---
title: "SSML - Predicting Builidng Age by Reflectance"
author: "Julia Ruiter"
date: "4/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PREPROCESSING

```{r}
#Loading Packages 
library("sf")
library("spdep")
library("tmap")
library("spatialreg")
library("dplyr")        # only works if both datasets NOT sf
library("sfheaders")    # convert from sf to df
library("speedglm")     # dealing with massive regressions
```
.
```{r}
#Reading the building info datasets - separately to avoid crash
utrecht_BAG <- read_sf("verblijsobject_utrecht.shp", stringsAsFactors = T)
```
.
```{r}
utrecht_PDOK <- read_sf("verblijfsobject_sampleset_1thru7combined.shp", stringsAsFactors = T)
```

Why are there 2 building information datasets?

Originally, we tried to download the BAG data available through PDOK.  This information contained everything we wanted to know about build year, but requests time out after around 1000 random datapoints, so aggregating this data yields an incomplete dataset.

To get the full dataset, we gained access to the full datapackage available on the UU servers.  However, it turns out that because this data originally comes from a different source, it has different metadata attached to it.  While now we have the full set of points (and gained information on building-usage), we are missing the build-year.

```{r}
head(utrecht_PDOK)
print('number of points gathered from PDOK set: ')
PDOK_count <- nrow(utrecht_PDOK)
PDOK_count

print('')
head(utrecht_BAG)
print('number of points gathered from full BAG set: ')
BAG_count <- nrow(utrecht_BAG)
BAG_count

print('')
print('Buildings missing')
BAG_count - PDOK_count
```
We have full data for 57.8% of the buildings in Utrecht if we combine these datasets.

The combined set will be used for training and testing, while the remaining unmatched buildings from the full BAG set will be what we predict.

First, we need to check crs and re-map if necessary to get them to match
```{r}
st_crs(utrecht_PDOK)
```
```{r}
st_crs(utrecht_BAG)
```
The CRS do NOT match, therefore we need to convert one.  Let's convert BAG's "Amersfoort" crs to the more common "WGS84"/EPSG 4326.

```{r}
utrecht_BAG_84 <- st_transform(utrecht_BAG, 4326)
# check that the crs is correct
st_crs(utrecht_BAG_84)
```

Now that they match, proceed with left join to append buildyear to 
```{r}
# DO NOT USE THIS CHUNK - DOES NOT APPEND METADATA
#predictset_Utrecht <- st_join(
#  utrecht_BAG_84,
#  utrecht_PDOK,
#  join = st_intersects,
#  left = TRUE,
#  largest = FALSE
#)
#head(predictset_Utrecht)
#nrow(predictset_Utrecht)
#summary(predictset_Utrecht)
```


To append the metadata from the PDOK set, we will convert it to a dataframe.
```{r}
utrecht_PDOK_df <- sf_to_df(utrecht_PDOK, fill = TRUE, unlist = NULL)
```

Then we can link the PDOK dataframe to the BAG coordinates by linking on column 'identifica'
```{r}
utrecht_joined <- left_join(utrecht_BAG_84, utrecht_PDOK_df, by='identifica')
head(utrecht_joined)
```
The last step is to make a new dataframe of only points where we have full information.  These IDs will be used for building the model!
```{r}
utrecht_testtrain <- utrecht_joined[!is.na(utrecht_joined$bouwjaar),]
# check number of points ~60'000
nrow(utrecht_testtrain)   
```

## SATELLITE DATA PREPROCESSING: TODO
import
clean
append to utrecht_testtrain



## MODEL
# NOTE:  NEED TO IMPORT SATELLITE DATA AND APPEND TO LM
Prepreocessing done!  Now we can make our prediction models!

```{r}
set.seed(42)

# split into 70/30 tor training and testing
sample <- sample.int(n = nrow(utrecht_testtrain), size = floor(.7*nrow(utrecht_testtrain)), replace = F)
train <- utrecht_testtrain[sample, ]
test  <- utrecht_testtrain[-sample, ]
```


## basic model:  no spatial autocorrelation
Vector is too large IF postcode is included ==> this indicates that linear regression may not work once additional satellite categories are added
```{r}
basic_linear_all <- lm(bouwjaar ~ gebruiks_1 + status.x 
                   , data=train)
summary(basic_linear_all)
```

Because a regular linear regression takes up too much memory, we will utilize a package called "speedglm"
```{r}
#library("speedglm")
#basic_linear <- speedlm(bouwjaar ~ gebruiks_1 + status.x 
#                        , data=train)
#summary(basic_linear)
```

## testing for spatial autocorrelation:
using the significant variables from the previous analysis, we can build a matrix OLS solution.  This
```{r}
# HOUSING EXAMPLE:  DELETE LATER -- hh is the merged data set
f1 <- houseValue ~ age +  nBedrooms
m1 <- lm(f1, data=hh)
summary(m1)
# the three lines above redo the analysis in the previous block - 


# OLS matrix calulations to get around memory issues
y <- matrix(hh$houseValue)            # matrix of lm outcome
X <- cbind(1, hh$age, hh$nBedrooms)   # matrix of lm inputs
ols <- solve(t(X) %*% X) %*% t(X) %*% y    # THIS IS SUPER IMPORTANT
rownames(ols) <- c('intercept', 'age', 'nBedroom')
ols
hh$residuals <- residuals(m1)

# check which areas have strong correlation
brks <- quantile(hh$residuals, 0:(grps-1)/(grps-1), na.rm=TRUE)
spplot(hh, "residuals", at=brks, col.regions=rev(brewer.pal(grps, "RdBu")), col="black")


# IFF we can get enough memory space to include postcodes (ONLY FOR VISUALS)
library(spdep)
nb <- poly2nb(hh)
nb[[21]] <- sort(as.integer(c(nb[[21]], 38)))
nb[[38]] <- sort(as.integer(c(21, nb[[38]])))
nb
## Neighbour list object:
## Number of regions: 58
## Number of nonzero links: 278
## Percentage nonzero weights: 8.263971
## Average number of links: 4.793103
par(mai=c(0,0,0,0))
plot(hh)
plot(nb, coordinates(hh), col='red', lwd=2, add=TRUE)

resnb <- sapply(nb, function(x) mean(hh$residuals[x]))
cor(hh$residuals, resnb)
## [1] 0.6311218
plot(hh$residuals, resnb, xlab='Residuals', ylab='Mean adjacent residuals')   # check for correlation - optional

lw <- nb2listw(nb)   # WEIGHTS MATRIX!  we need this outcome regardless


moran.mc(hh$residuals, lw, 999)  # THIS IS WHAT WE NEED TO TEST FOR SPATIAL CORRELATION


#######################################################################
# skipping the above visuals:
# now add spatial lag:
m1s = lagsarlm(f1, data=hh, lw, tol.solve=1.0e-30)
summary(m1s)
hh$residuals <- residuals(m1s)
moran.mc(hh$residuals, lw, 999)
```






## refined model: with spatial autocorrelation







## DATA SUBSET IF FULL DATA REQUIRES TOO MUCH MEMORY
...okay, trying first with a subset of the data:  let's use 10% of the available data:
```{r}
sample_train_10 <- sample.int(n = nrow(train), size = floor(.1*nrow(train)), replace = F)
sample_test_10  <- sample.int(n = nrow(test), size = floor(.1*nrow(test)), replace = F)

train_10 <- train[sample_train_10, ]
test_10  <- test[sample_test_10, ]
```


```{r}
basic_linear <- lm(bouwjaar ~ gebruiks_1 + status.x 
                   , data=train_10)
summary(basic_linear)
```
"other" and "residential" have a stronger correlation with builyear.  This makes sense as "other" may be museums which are likely old building, while neighbourhoods are built usually at the same time, so there would be a strong relation.

NOTE!  I am NOT using postcode because postcode is specific to Utrecht, therefore this model would not be generalisable to other locations!

```{r}

```
